%https://en.wikibooks.org/wiki/LaTeX/Text_Formatting
\documentclass[10pt,a4paper]{report}
\usepackage[pdftex]{graphicx}
\usepackage{subfigure}            % Multiple figures per float
\usepackage{placeins}             % Fix float loactions with \FloatBarrier
\usepackage{color}                % Color definitions for boxes
\usepackage{multirow}             % Multiple rows per cell in a table
\usepackage{verbatim}             % Long verbatim sections
\usepackage{titlesec}             % Easy modification the chapter format
\usepackage{fancyhdr}             % Easy modification of header and footer
\usepackage{hyperref}			  % Custom hyperlinks

\renewcommand{\arraystretch}{1.2} % Extra space in tables
\parindent0mm                     % New paragraphs start without indentation
\setlength{\parskip}{1em}         % And with a blank line in between

% Redefine chapters to remove the "Chapter" word
\titleformat{\chapter}
  {\normalfont\LARGE\bfseries}{\thechapter}{1em}{}
\titlespacing*{\chapter}{0pt}{3.5ex plus 1ex minus .2ex}{2.3ex plus .2ex}

% Setup hyperlink format in document
\hypersetup{
    colorlinks=true, %set true if you want colored links
    linkcolor=blue,  %choose some color if you want links to stand out
    citecolor=blue,  %choose some color if you want lcitation to stand out
    filecolor=black, % etc...
    urlcolor=blue
}

% Define header and footer
\pagestyle{fancy}
\fancyhf{}
\lhead{Kent Milfeld}
\rhead{Maskeraid User Guide}
\lfoot{Maskeraid}
\rfoot{\thepage}

% Define header and footer for first page in chapter
\fancypagestyle{plain}{
\fancyhf{}
\lhead{Kent Milfeld}
\rhead{Maskeraid User Guide}
\lfoot{Maskeraid}
\rfoot{\thepage}
}

% Gray boxes for optional material
\definecolor{LightGrey}{gray}{.85}
\setlength{\fboxrule}{1pt}
\setlength{\fboxsep}{6pt}
\newcommand{\IntroBox}[1]{
  %\fcolorbox[rgb]{0,0,0}{0.95,0.95,0.95}{
    \fcolorbox{black}{LightGrey}{
    \begin{minipage}{0.94\linewidth}
      %\textbf{Introduction}
      #1
    \end{minipage}
  }
}




\begin{document}

\begin{titlepage}
\thispagestyle{empty}	%don't include number on cover
%\begin{flushleft}
%\begin{figure}
%\includegraphics[width=0.2\textwidth]{gplv3-127x51.png}
%\end{figure}
%\end{flushleft}
\verb+ +
\vspace{1em}
\begin{flushright}
\huge\bf Maskeraid v0.9\\
\rule{\textwidth}{4pt}
\large{\bf Mask Aid for Affinity Binding (Scheduling)\\
Document Revision 1.0\\
\today}
\end{flushright}

%Back of cover page (copyright)
\newpage
\thispagestyle{empty}
\begin{flushleft}
Kent Milfeld \\
\verb+milfeld@tacc.utexas.edu+\\
\vspace{0.5em}
High Performance Computing \\
Texas Advanced Computing Center\\
The University of Texas at Austin\\
\vspace{1cm}
Copyright 2015 The University of Texas at Austin.
\end{flushleft}
\newpage
\end{titlepage}

\begin{abstract}

Maskeraid is a set of tools to help researchers discover the binding of their processes(MPI ranks)/threads(OpenMP threads) in a parallel environment. Maskeraid has the following components:

\begin{itemize}
\item Stand-alone executables to report default masks for execution of an OpenMP, MPI or Hybrid run.
\item API for instrumenting applications to report affinity masks.
\item Utilities: timers, process binder, routine to set core load.
\end{itemize}

Throughout the text we have used text boxes to highlight important information. These boxes look like this:

\IntroBox{These gray boxes contain highlighted material for each section and chapter.}

Our intention is to create a simple runtime resource monitoring tool that provides both simple to understand high level information to the user, as well as detailed statistics for in-depth analysis. We welcome all feedback. Feedback that includes suggestions for improvement in the usability, reliability, and accuracy of this tool is particularly welcome. 


If you use Maskeraid, you can cite it:

Kent F Milfeld. ``Maskeraid: An Aid for Report Affinity Masks''. \cite{maskeraid}

\end{abstract}

\tableofcontents

\chapter{Installation}
Maskeraid is easy to build. A make command will build executables and a library for those who want to instrument their code.

Download the github repository \footnote{\href{https://github.com/TACC/maskeraid}{https://github.com/TACC/maskeraid}} by clicking on the ``Download ZIP'' file, and expand it in a convenient location.

\begin{verbatim}
    unzip  maskeraid-master.zip
\end{verbatim}

You can also clone the git repository:

\begin{verbatim}
    git clone https://github.com/TACC/maskeraid
\end{verbatim}

This will create a top level directory called \verb+maskeraid+, with subdirectories \verb+docs+ and \verb+src+. Change directory to the top level of maskeraid and edit \verb+install.sh+ to reflect your choice for the installation directory. The variables to modify are:

\begin{table}[h]
\centering
\label{tab:env}
\begin{tabular}{|l|l|l|}
\hline
\bf{Variable}	& \bf{Description}                          & \bf{Default}\\\hline
MASKERAID\_DIR  & Absolute path to installation directory   & . \\\hline
\end{tabular}
\end{table}

Once these variables are set the tool can be installed by using:

\begin{verbatim}
    ./install.sh
\end{verbatim}

The executables will be placed in the \verb+maskeraid/bin+ directory and the library will be placed in \verb+maskeraid/lib+  unless the \verb+MASKERAID_DIR=...+ has been modified in the Makefile.

\FloatBarrier
\chapter{Design and Implementation}

Maskeraid was originally designed just to display the affinity (mask) of processes that
are launched by an MPI executable launcher (mpirun, mpiexec, TACC ibrun, etc.).
When maskeraid is built, it creates mpi\_whereami. This MPI executable
should be substituted for an mpi application executable of the researcher in a test run
to discover how the launcher will distributed the application's tasks (ranks).

That is, in a job script the user would simple replace the application executable name
with mpi\_whereami (you may need to include a path to the executable location), as
shown below:

\IntroBox{
        \#!/bin/bash \\
        \#SLURM  -n 16 -N 1 \\
        ... \\
        \#Batch Script for TACC machine \\
        ... \\
        \#ibrun ./my\_mpi\_exec \\
        ibrun ./mpi\_whereami \\
           ibrun ./mpi\_whereami \\
              ibrun ./mpi\_whereami
                 ibrun ./mpi\_whereami \\
              ibrun ./mpi\_whereami
}

The \verb+mpi_whereami+ executable puts a load on the ranks for 10 seconds (default), 
so that \verb+top+ can be run on
a node to observe the core occupation.  (Execute \verb+top+, and then press the
1 key to change the display to show the percentage load on each core.  You can
reset the load time by setting the MASKERAID\_LOAD\_SECONDS environment
variable to a positive integer (units are seconds).

SPECIAL OUTPUT 1  reference and table.

The resulting output in Output 1. shows a matrix with rows labeled by the ranks
of a mpi\_whereami MPI tasks for the above launch on Stampede, the columns represent 
CPU\_ids (core id) with positions representing the the CPU\_ids from 0 to ntasks-1, 
beginning with the first column of the matrix.

A non-hyphen character in the matrix means that
the mpi rank (row value) is allowed to execute in the CPU\_id (column value). In the
output below,
rank 0 is "mapped" to core (CPU\_id) 0, rank 1 is mapped to
core (CPU\_id) 1, etc., so as to have each rank execute on only one core (CPU\_id).

To make it easy to determine the CPU\_id value in the matrix, the first digit of
CPU\_id (core) number is printed in the matrix.  For CPU\_ids larger than 9, the
The "10s" position value can be read from the column header demarking the set
by bars (|). For instance the second set of values \{0, 1, 2, 3, 4, 5\} along the
diagonal represent CPU\_id's \{ 10, 11, 12, 13, 14, and 15\}-- the "10" in the header
"|       10     |" should be added to the unit digits within the matrix to get the
CPU\_id value.


\section{Statistics Collected}
REMORA collects a set of statistics that are useful in many different scenarios when profiling an application. The data collected by REMORA consists of:

\begin{itemize}
\item Detailed timing of the application.
\item CPU utilization.
\item Memory utilization.
\item NUMA information.
\item I/O information (file system load and Lustre traffic).
\item Network information (topology and InfiniBand traffic).
\end{itemize}

Dynamic information is collected every REMORA\_PERIOD seconds. The following describes the data collected in more detail. 

\paragraph{CPU}
The application reports the average CPU usage of the last second (independently of the value specified for REMORA\_PERIOD). This information is very important for applications that use OpenMP, where it is possible to easily analyze how the cores are being used. It also allows to check for a correct pinning of threads to the cores: not pinning processes could lead to threads floating between cores, which will be show up in this report. MPI applications can also benefit from this information.

\paragraph{Memory}
One of the most recurring questions for HPC users is "how can I know how much memory my job is using?". Trying to answer that question, REMORA collects the most relevant statistics regarding memory usage every REMORA\_PERIOD seconds:

\begin{itemize}
\item Virtual Memory (and Max Virtual Memory): this is a very important value as the OOM (out of memory) killer will use it to kill the application if needed.
\item Resident Memory (and Max Resident Memory): physical memory used by the application.
\item Shared memory: applications have access to shared memory by means of /dev/shm. Any file that is put there counts towards the memory used by the application, so the application reports this usage.
\item Total free memory: this will take into account the memory not being used by the application, the libraries needed by the application, and the OS.
\end{itemize}

Data is collected from /proc/\textless pid\textgreater /status for all of these except shared memory, which can be obtained from /dev/shm. Memory usage for all user processes is aggregated and written to a single file per node involved in the execution. At the end of the run the maximum values for memory utilization (and minimum value of total free memory) are aggregated into a single file.

When Xeon Phi co-processors are used as part of a symmetric execution model, each Xeon Phi is treated as a separate node and the same memory information is collected from Phi and from host CPU. Individual files are maintained for each node and Phi and the per node aggregated summary is provided individually for nodes and Xeon Phi devices, since their available memory tends to be different.

\paragraph{NUMA}
As it is well known, NUMA (Non-Uniform Memory Access) can have a large impact on the overall performance of an application. Sometimes small changes in the code can lead to large improvements once it has been discovered that NUMA was imposing a penalty over the application. Our tool reports how memory is being used in each socket and it also collects the number of NUMA hits and misses. The information is extracted from the numastat tool: numastat is called only once on each collecting period; the output of numastat is then analyzed and several different fields are used for the statistics:

\begin{itemize}
	\item Number of hits: total number of memory access hits. 
	\item Number of misses: total number of memory access misses. 
	\item Number of hits in the current node: if the data that the application was looking for is in the same of node where the core requesting that data is located, it produces a hit in the current NUMA node.
	\item Number of hits in the other node: when the data required is in cache, but in the other NUMA node.
	\item Total memory free/used on each node.
\end{itemize}


\paragraph{xltop} \cite{xltop}. xltop is a Lustre monitor developed at TACC that can be freely downloaded and installed. It provides statistics about file system utilization, including I/O requests per second and read and write volumes in MB/s. Originally, xltop was a continuous monitoring client that connects to other tools (xltop-master, xltop-servd, and xltop-clusd) to retrieve the information statistics from every MDS (metadata server) and OSS (Object Storage Server). This is a tool used by administrators to specifically look for very heavy I/O applications that can damage the stability of the file system. We have modified the original xltop code so that it can be invoked by any user to only check for the Lustre I/O statistics of the job being analyzed. Also, while xltop originally used ncurses to show the results, we have now implemented a version that collects the statistics in a file in order to simplify the analysis of the results.

\paragraph{Lustre Stats}
Our tool collects information regarding the data transferred by Lustre on each node used by the job while running. Normally, these statistics do not provide much information to the users. However, they are very useful if there was a problem in the file system while the job was being analyzed, as the number of messages dropped will significantly increase. The following Lustre information is collected:

\begin{itemize}
\item Number of currently active Lustre messages. It also includes a highwater mark of this value.
\item Messages sent/received: total number of Lustre messages sent/received by the current node.
\item Messages dropped: number of Lustre messages that failed to be delivered to the destination.
\item Bytes sent/received: total number of bytes sent/received on Lustre messages.
\end{itemize}


\paragraph{InfiniBand Packets}
Number of packets transmitted using InfiniBand. This data can be used to get extra information regarding how the communication in parallel applications takes place. In particular, the time series can be used to correlate high network activity levels with sections of the code, and those sections can be revised for possible optimization.

\FloatBarrier
\chapter{Using Remora}

\section{Collecting Data}
After making sure the remora \verb+bin+ directory is in your \verb+PATH+ and the remora \verb+lib+ directory is in your \verb|LD_LIBRARY_PATH|

\section{Post-Processing}
All the data is collected in a set of files with the statistics organized in columns. Users can take those files and run any postprocessing tool that they develop. However, for simplicity, REMORA already provides a plotting script called \verb+remora_post+ that takes all the statistics generated during collection and generates a number of plots. These plots show the most relevant information previously collected and represent a visual alternative while analyzing the results. This is a Python script that requires Numpy, matplotlib and blockdiag. The script can be called from the batch script or from the login nodes after the job has finished. In this second scenario, the script requires the id of the job to be analyzed as argument.

\section{Execution Customization}

REMORA is configurable in terms of the amount and type of data collected, but sensible defaults are provided to simplify its use. By default the statistics are collected every ten seconds.

REMORA provides two different running modes and it also allows the users to specify how frequently the data is collected. A verbose mode is provided mostly for troubleshooting and should not be used by default. The behavior of the application is controlled via three environment variables:

\begin{itemize}
	\item REMORA\_PERIOD: Time in seconds between consecutive data records. This is the time from the end of a collection event until the start of the next collection event. Depending on the platform where the tool is running, the overhead introduced by the application can make the duration of the collection event to vary, in which case there will be less data points in the collected results than expected. However, in the systems that we have tested the overhead of the application is small enough that the total number of collection points (CP) is almost equal to $CP = ET/RP$ where ET is the execution time (in seconds) and RP is the period (in seconds).

	\item REMORA\_MODE: this variable accepts two possible values (FULL and BASIC). The FULL mode runs all the tests that the tool allows. The BASIC mode only reports memory and cpu usage. This is the recommended mode when the users now that the application of interest does not create problems in the distributed file system. The tool that checks for the file system load \cite{xltop} is the main contributor to the small overhead introduced by the tool. When this report is not required, the overall overhead of the tool is very small.
	
	\item REMORA\_VERBOSE: Enable (1) or disable (0) verbose mode. Default is disabled.
\end{itemize}

\FloatBarrier
\addcontentsline{toc}{section}{\bf References}
\begin{thebibliography}{00}

\bibitem{remora} C. Rosales, A. G\'{o}mez-Iglesias, A. Predoehl. ``REMORA: a Resource Monitoring Tool for Everyone''. HUST2015 November 15-20, 2015, Austin, TX, USA. DOI: \href{http://dx.doi.org/10.1145/2834996.2834999}{10.1145/2834996.2834999}

\bibitem{xltop}xltop is a continuous Lustre monitor with batch scheduler intergation: \href{https://github.com/jhammond/xltop}{https://github.com/jhammond/xltop}


\end{thebibliography}

\end{document}
